{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bb8a0fd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.layers import *\n",
    "from keras.models import Sequential\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from keras.callbacks import EarlyStopping,CSVLogger\n",
    "from tensorflow.keras.utils import plot_model\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "54e2b404",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the height and width to which each video frame will be resized in our dataset.\n",
    "IMAGE_HEIGHT , IMAGE_WIDTH = 64, 64\n",
    "\n",
    "SEQUENCE_LENGTH = 20\n",
    "\n",
    "DATASET_DIR = \"./Fire_Complete/\"\n",
    " \n",
    "CLASSES_LIST = [\"Fire\",\"noFire\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "72f95780",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 62, 62, 32)        896       \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2D  (None, 31, 31, 32)       0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 29, 29, 64)        18496     \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPooling  (None, 14, 14, 64)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 12, 12, 128)       73856     \n",
      "                                                                 \n",
      " max_pooling2d_2 (MaxPooling  (None, 6, 6, 128)        0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 4608)              0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 64)                294976    \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 2)                 130       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 388,354\n",
      "Trainable params: 388,354\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model = tf.keras.Sequential([\n",
    "    Conv2D(32, (3, 3), activation='relu', input_shape=(IMAGE_HEIGHT, IMAGE_WIDTH, 3)),\n",
    "    MaxPooling2D((2, 2)),\n",
    "    Conv2D(64, (3, 3), activation='relu'),\n",
    "    MaxPooling2D((2, 2)),\n",
    "    Conv2D(128, (3, 3), activation='relu'),\n",
    "    MaxPooling2D((2, 2)),\n",
    "    Flatten(),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dense(len(CLASSES_LIST), activation='sigmoid')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "model.summary()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8aab2de4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset():    \n",
    "    features = []\n",
    "    labels = []\n",
    "    for class_index, class_name in enumerate(CLASSES_LIST):\n",
    "\n",
    "        print(f'Extracting Data of Class: {class_name}')\n",
    "\n",
    "        # Get the list of video files present in the specific class name directory.\n",
    "        files_list = os.listdir(os.path.join(DATASET_DIR, class_name))\n",
    "        for img in files_list:\n",
    "            path = os.path.join(DATASET_DIR, class_name, img)\n",
    "            image = cv2.imread(path)\n",
    "            resized_img = cv2.resize(image, (IMAGE_HEIGHT, IMAGE_WIDTH))\n",
    "            # Normalize the resized frame\n",
    "            normalized_img = resized_img / 255\n",
    "            features.append(normalized_img)\n",
    "            labels.append(class_index)\n",
    "    features = np.asarray(features)\n",
    "    labels = np.array(labels)\n",
    "    return features,labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0352bd6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting Data of Class: Fire\n",
      "Extracting Data of Class: noFire\n"
     ]
    }
   ],
   "source": [
    "features, labels = create_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1a9a9826",
   "metadata": {},
   "outputs": [],
   "source": [
    "one_hot_encoded_labels = to_categorical(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7728ac81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(925, 64, 64, 3)\n",
      "(925, 2)\n"
     ]
    }
   ],
   "source": [
    "print((features.shape))\n",
    "print((one_hot_encoded_labels.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "adff25b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_train, features_test, labels_train, labels_test = train_test_split(features, one_hot_encoded_labels, test_size = 0.2,\n",
    "                                                                            shuffle = True, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a75ecdff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(740, 64, 64, 3) (740, 2)\n",
      "(185, 64, 64, 3) (185, 2)\n"
     ]
    }
   ],
   "source": [
    "print(features_train.shape,labels_train.shape )\n",
    "print(features_test.shape, labels_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "82551817",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "19/19 [==============================] - 3s 130ms/step - loss: 0.5941 - accuracy: 0.6740 - val_loss: 0.5268 - val_accuracy: 0.6689\n",
      "Epoch 2/20\n",
      "19/19 [==============================] - 2s 121ms/step - loss: 0.3857 - accuracy: 0.8243 - val_loss: 0.2821 - val_accuracy: 0.8851\n",
      "Epoch 3/20\n",
      "19/19 [==============================] - 2s 125ms/step - loss: 0.3080 - accuracy: 0.8818 - val_loss: 0.3836 - val_accuracy: 0.8716\n",
      "Epoch 4/20\n",
      "19/19 [==============================] - 3s 154ms/step - loss: 0.2472 - accuracy: 0.9088 - val_loss: 0.3810 - val_accuracy: 0.8716\n",
      "Epoch 5/20\n",
      "19/19 [==============================] - 12s 639ms/step - loss: 0.2572 - accuracy: 0.8919 - val_loss: 0.2736 - val_accuracy: 0.9189\n",
      "Epoch 6/20\n",
      "19/19 [==============================] - 10s 524ms/step - loss: 0.2169 - accuracy: 0.9122 - val_loss: 0.2311 - val_accuracy: 0.8851\n",
      "Epoch 7/20\n",
      "19/19 [==============================] - 6s 318ms/step - loss: 0.1923 - accuracy: 0.9274 - val_loss: 0.2551 - val_accuracy: 0.9257\n",
      "Epoch 8/20\n",
      "19/19 [==============================] - 5s 237ms/step - loss: 0.1751 - accuracy: 0.9240 - val_loss: 0.2413 - val_accuracy: 0.8919\n",
      "Epoch 9/20\n",
      "19/19 [==============================] - 4s 234ms/step - loss: 0.1343 - accuracy: 0.9645 - val_loss: 0.2913 - val_accuracy: 0.9189\n",
      "Epoch 10/20\n",
      "19/19 [==============================] - 5s 254ms/step - loss: 0.1103 - accuracy: 0.9645 - val_loss: 0.2394 - val_accuracy: 0.9054\n",
      "Epoch 11/20\n",
      "19/19 [==============================] - 4s 201ms/step - loss: 0.1247 - accuracy: 0.9578 - val_loss: 0.2417 - val_accuracy: 0.9054\n",
      "Epoch 12/20\n",
      "19/19 [==============================] - 3s 160ms/step - loss: 0.0887 - accuracy: 0.9628 - val_loss: 0.3174 - val_accuracy: 0.9189\n",
      "Epoch 13/20\n",
      "19/19 [==============================] - 3s 161ms/step - loss: 0.0830 - accuracy: 0.9747 - val_loss: 0.2551 - val_accuracy: 0.9054\n",
      "Epoch 14/20\n",
      "19/19 [==============================] - 3s 136ms/step - loss: 0.0739 - accuracy: 0.9831 - val_loss: 0.4294 - val_accuracy: 0.9122\n",
      "Epoch 15/20\n",
      "19/19 [==============================] - 3s 133ms/step - loss: 0.0576 - accuracy: 0.9797 - val_loss: 0.2979 - val_accuracy: 0.8986\n",
      "Epoch 16/20\n",
      "19/19 [==============================] - 2s 125ms/step - loss: 0.0453 - accuracy: 0.9882 - val_loss: 0.3613 - val_accuracy: 0.9189\n",
      "Epoch 17/20\n",
      "19/19 [==============================] - 3s 157ms/step - loss: 0.0339 - accuracy: 0.9916 - val_loss: 0.5434 - val_accuracy: 0.8986\n",
      "Epoch 18/20\n",
      "19/19 [==============================] - 2s 126ms/step - loss: 0.0326 - accuracy: 0.9899 - val_loss: 0.3790 - val_accuracy: 0.9189\n",
      "Epoch 19/20\n",
      "19/19 [==============================] - 2s 130ms/step - loss: 0.0214 - accuracy: 0.9949 - val_loss: 0.4263 - val_accuracy: 0.9189\n",
      "Epoch 20/20\n",
      "19/19 [==============================] - 38s 2s/step - loss: 0.0168 - accuracy: 0.9949 - val_loss: 0.4458 - val_accuracy: 0.9189\n"
     ]
    }
   ],
   "source": [
    "# Create Early Stopping Callback to monitor the accuracy\n",
    "early_stopping_callback = EarlyStopping(monitor = 'val_accuracy', patience = 10, restore_best_weights = True)\n",
    "\n",
    "# Create ReduceLROnPlateau Callback to reduce overfitting by decreasing learning\n",
    "reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss',\n",
    "                                                  factor=0.6,\n",
    "                                                  patience=5,\n",
    "                                                  min_lr=0.00005,\n",
    "                                                  verbose=1)\n",
    " \n",
    "# Compiling the model \n",
    "#model.compile(loss = 'binary_crossentropy', optimizer = 'sgd', metrics = [\"accuracy\"])\n",
    " \n",
    "csv_logger = CSVLogger('training.log', separator=',', append=False)\n",
    "   \n",
    "# Fitting the model \n",
    "model_history = model.fit(x = features_train, y = labels_train, epochs = 20,\n",
    "                                             shuffle = True, validation_split = 0.2)\n",
    "#, callbacks =   [early_stopping_callback,reduce_lr,csv_logger]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "26b1f4d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 1s 66ms/step - loss: 0.6337 - accuracy: 0.8486\n"
     ]
    }
   ],
   "source": [
    "#model.fit(train_data, epochs=10, validation_data=val_data)\n",
    "#test_data = dataset['test']\n",
    "model_evaluation_history = model.evaluate(features_test,labels_test)\n",
    "#print('Test accuracy:', accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "af8f36ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fire_model_84.h5\n"
     ]
    }
   ],
   "source": [
    "# Get the loss and accuracy from model_evaluation_history.\n",
    "model_evaluation_loss, model_evaluation_accuracy = model_evaluation_history\n",
    "   \n",
    "model_file_name = f'fire_model_{int((model_evaluation_accuracy)*100)}.h5'\n",
    "print(model_file_name)\n",
    "# Save the Model.\n",
    "model.save(model_file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "56ebaf08",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "f48d81d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model=load_model('fire_model_84.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ae15f226",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "7bad1ee3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 1s 56ms/step\n"
     ]
    }
   ],
   "source": [
    "predicted=model.predict(features_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f67b62ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(185, 2)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(185, 2)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(predicted.shape)\n",
    "labels_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a41c438f",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_new=np.zeros((predicted.shape[0],1))\n",
    "actual=np.zeros((predicted.shape[0],1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "6b0eb719",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(actual)):\n",
    "    if labels_test[i][0]> labels_test[i][1]:\n",
    "        actual[i][0]=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "efaa37aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(predicted_new)):\n",
    "    if predicted[i][0]>predicted[i][1]:\n",
    "        predicted_new[i][0]=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "a169fe9c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "confusion_matrix = metrics.confusion_matrix(actual, predicted_new)\n",
    "\n",
    "cm_display = metrics.ConfusionMatrixDisplay(confusion_matrix = confusion_matrix, display_labels = ['Fight', 'No Fight'])\n",
    "\n",
    "cm_display.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "b4b5005e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision is: 0.8191489361702128\n",
      "Recall is: 0.875\n",
      "F1 score is: 0.8461538461538463\n"
     ]
    }
   ],
   "source": [
    "print(f'Precision is: {metrics.precision_score(actual,predicted_new)}')\n",
    "print(f'Recall is: {metrics.recall_score(actual,predicted_new)}')\n",
    "print(f'F1 score is: {metrics.f1_score(actual,predicted_new)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "9d8d59a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_video(video_file_path):\n",
    " \n",
    "    video_reader = cv2.VideoCapture(video_file_path)\n",
    " \n",
    "    # Get the width and height of the video.\n",
    "    original_video_width = int(video_reader.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    original_video_height = int(video_reader.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    " \n",
    "    # Declare a list to store video frames we will extract.\n",
    "    frames_list = []\n",
    "    \n",
    "    # Store the predicted class in the video.\n",
    "    predicted_class_name = ''\n",
    " \n",
    "    # Get the number of frames in the video.\n",
    "    video_frames_count = int(video_reader.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    " \n",
    "    # Calculate the interval after which frames will be added to the list.\n",
    "    skip_frames_window = max(int(video_frames_count/SEQUENCE_LENGTH),1)\n",
    " \n",
    "    # Iterating the number of times equal to the fixed length of sequence.\n",
    "    for frame_counter in range(SEQUENCE_LENGTH):\n",
    " \n",
    "        # Set the current frame position of the video.\n",
    "        video_reader.set(cv2.CAP_PROP_POS_FRAMES, frame_counter * skip_frames_window)\n",
    " \n",
    "        success, frame = video_reader.read() \n",
    " \n",
    "        if not success:\n",
    "            break\n",
    " \n",
    "        # Resize the Frame to fixed Dimensions.\n",
    "        resized_frame = cv2.resize(frame, (IMAGE_HEIGHT, IMAGE_WIDTH))\n",
    "        \n",
    "        # Normalize the resized frame.\n",
    "        normalized_frame = resized_frame / 255\n",
    "        \n",
    "        # Appending the pre-processed frame into the frames list\n",
    "        frames_list.append(normalized_frame)\n",
    " \n",
    "    predicted_labels_probabilities = []\n",
    "    predicted_label_lst = []\n",
    "    # Passing the  pre-processed frames to the model and get the predicted probabilities.\n",
    "    avg = [0,0]\n",
    "    for i in range(SEQUENCE_LENGTH):\n",
    "        \n",
    "        predicted_labels_probabilities.append(model.predict(np.expand_dims(frames_list[i], axis = 0))[0])\n",
    "        avg[0] += predicted_labels_probabilities[len(predicted_labels_probabilities)-1][0]\n",
    "        avg[1] += predicted_labels_probabilities[len(predicted_labels_probabilities)-1][1]\n",
    "        \n",
    "        # Get the index of class with highest probability.\n",
    "        #predicted_label_lst.append(np.argmax(predicted_labels_probabilities))\n",
    "    \n",
    "    #count_0 = predicted_label_lst.count(0)\n",
    "    #count_1 = predicted_label_lst.count(1)\n",
    "    \n",
    "    avg[0] /= 20\n",
    "    avg[1] /= 20\n",
    "    if avg[0] > avg[1]:\n",
    "        predicted_label = 0\n",
    "    else:\n",
    "        predicted_label = 1\n",
    "    \n",
    "    # Get the class name using the retrieved index.\n",
    "    predicted_class_name = CLASSES_LIST[predicted_label]\n",
    "    \n",
    "    # Display the predicted class along with the prediction confidence.\n",
    "    print(f'Predicted: {predicted_class_name}\\nConfidence: {avg[predicted_label]}')\n",
    "        \n",
    "    video_reader.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "a4820690",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 47ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 48ms/step\n",
      "1/1 [==============================] - 0s 50ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 49ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "Predicted: noFire\n",
      "Confidence: 0.9430145367980003\n"
     ]
    }
   ],
   "source": [
    "# Specifying video to be predicted\n",
    "input_video_file_path = \"test3.mp4\"\n",
    "\n",
    "# Perform Single Prediction on the Test Video.\n",
    "predict_video(input_video_file_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc63632c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
